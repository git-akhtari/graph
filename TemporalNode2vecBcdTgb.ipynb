{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPKqtpqKHpZiyajKC6mLZ68",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/git-akhtari/graph/blob/main/TemporalNode2vecBcdTgb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqkvBzaAIZC4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "!pip install torch-geometric\n",
        "from torch_geometric.loader import TemporalDataLoader\n",
        "!pip install py-tgb\n",
        "from tgb.utils.utils import get_args\n",
        "from tgb.linkproppred.dataset_pyg import PyGLinkPropPredDataset\n",
        "from tgb.linkproppred.evaluate import Evaluator\n",
        "import node2vec_walks\n",
        "\n",
        "# Generate Walks\n",
        "def generate_walks(nx_G, length, p, q):\n",
        "  G = node2vec_walks.Graph(nx_G, is_directed=False, p=p, q=q)\n",
        "##G = Graph(nx_G, is_directed=False, p=p, q=q)\n",
        "  G.preprocess_transition_probs()\n",
        "  return G.simulate_walks(num_walks=1, walk_length=length)\n",
        "\n",
        "# def generate_walks(graph, walk_length):\n",
        "#     walks = []\n",
        "#     for node in graph.nodes():\n",
        "#         walk = [node]\n",
        "#         for _ in range(walk_length - 1):\n",
        "#             neighbors = list(graph.neighbors(walk[-1]))\n",
        "#             if len(neighbors) > 0:\n",
        "#                 walk.append(np.random.choice(neighbors))\n",
        "#             else:\n",
        "#                 break\n",
        "#         walks.append(walk)\n",
        "#     return walks\n",
        "\n",
        "# def generate_walks(graph, length, p):\n",
        "# walks = []\n",
        "# for node in graph.nodes():\n",
        "#     walk = [node]\n",
        "#     while len(walk) < length:\n",
        "#         current_node = walk[-1]\n",
        "#         neighbors = list(graph.neighbors(current_node))\n",
        "#         if not neighbors:\n",
        "#             break\n",
        "#         if random.random() < p:\n",
        "#             walk.append(walk[-2] if len(walk) > 1 else random.choice(neighbors))\n",
        "#         else:\n",
        "#             next_node = random.choice(neighbors)\n",
        "#             walk.append(next_node)\n",
        "#     walks.append(walk)\n",
        "# return walks\n",
        "\n",
        "# Compute PPMI Matrix\n",
        "def compute_ppmi(walks, window_size, num_nodes):\n",
        "    cooccurrence_matrix = np.zeros((num_nodes, num_nodes))\n",
        "    for walk in walks:\n",
        "        for i, node in enumerate(walk):\n",
        "            for j in range(max(i - window_size, 0), min(i + window_size + 1, len(walk))):\n",
        "                if i != j:\n",
        "                    cooccurrence_matrix[node, walk[j]] += 1\n",
        "    row_sums = np.sum(cooccurrence_matrix, axis=1, keepdims=True)\n",
        "    ppmi_matrix = np.log((cooccurrence_matrix * np.sum(row_sums)) / (row_sums @ row_sums.T) + 1e-8)\n",
        "    ppmi_matrix[ppmi_matrix < 0] = 0\n",
        "    return torch.tensor(ppmi_matrix, dtype=torch.float32)\n",
        "\n",
        "def _bcd_step(Yt, Ut, Wp, Wn, gamma, llambda, tau, idx):\n",
        "    UtU = Ut.T @ Ut\n",
        "    r = UtU.shape[0]\n",
        "\n",
        "    A = UtU + (gamma + llambda + 2 * tau) * torch.eye(r, device=Ut.device)\n",
        "    B = Yt @ Ut + gamma * Ut[idx, :] + tau * (Wp + Wn)\n",
        "\n",
        "    return torch.linalg.solve(A, B.T).T\n",
        "\n",
        "def construct_graph_from_batch(batch):\n",
        "    import networkx as nx\n",
        "    G = nx.DiGraph()\n",
        "    for src, dst in zip(batch.src.cpu().numpy(), batch.dst.cpu().numpy()):\n",
        "        G.add_edge(src, dst)\n",
        "    return G\n",
        "\n",
        "# Initialize embeddings\n",
        "def initialize_parameters(num_nodes, embedding_dim):\n",
        "    U = torch.randn(num_nodes, embedding_dim, device=device, requires_grad=False)\n",
        "    W = torch.randn(num_nodes, embedding_dim, device=device, requires_grad=False)\n",
        "    return U, W\n",
        "\n",
        "# Training function with BCD\n",
        "def train_with_bcd(train_loader, num_nodes, walk_length, window_size, lambda_reg, tau_reg, gamma_reg, U, W, device):\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Build graph for current batch\n",
        "        graph = construct_graph_from_batch(batch)\n",
        "\n",
        "        # Generate walks and compute PPMI\n",
        "        walks = generate_walks(graph, walk_length,0.1,0.1)\n",
        "        PPMI = compute_ppmi(walks, window_size, num_nodes)\n",
        "\n",
        "        # Block Coordinate Descent (BCD)\n",
        "        for idx in range(num_nodes):\n",
        "            if idx > 0:\n",
        "                Wp = W[idx - 1, :]\n",
        "                Up = U[idx - 1, :]\n",
        "            else:\n",
        "                Wp = torch.zeros_like(W[idx, :])\n",
        "                Up = torch.zeros_like(U[idx, :])\n",
        "\n",
        "            if idx < num_nodes - 1:\n",
        "                Wn = W[idx + 1, :]\n",
        "                Un = U[idx + 1, :]\n",
        "            else:\n",
        "                Wn = torch.zeros_like(W[idx, :])\n",
        "                Un = torch.zeros_like(U[idx, :])\n",
        "\n",
        "            # Update embeddings for current node\n",
        "            W[idx, :] = _bcd_step(PPMI[idx, :], U, Wp, Wn, gamma_reg, lambda_reg, tau_reg, idx)\n",
        "            U[idx, :] = _bcd_step(PPMI[idx, :], W, Up, Un, gamma_reg, lambda_reg, tau_reg, idx)\n",
        "\n",
        "        # Compute loss (optional)\n",
        "        loss = torch.norm(PPMI - U @ W.T, p='fro')  # Frobenius norm\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(train_loader)\n",
        "\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Parameters\n",
        "    DATA = \"tgbl-wiki\"\n",
        "    BATCH_SIZE = 200\n",
        "    embedding_dim = 128\n",
        "    teta = 1\n",
        "    window_size = 5\n",
        "    walk_length = 10\n",
        "    lambda_reg = 0.1\n",
        "    tau_reg = 0.1\n",
        "    gamma_reg = 0.1\n",
        "    num_epochs = 5\n",
        "\n",
        "    # Device setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = PyGLinkPropPredDataset(name=DATA, root=\"datasets\")\n",
        "    train_mask = dataset.train_mask\n",
        "    val_mask = dataset.val_mask\n",
        "    test_mask = dataset.test_mask\n",
        "    data = dataset.get_TemporalData()\n",
        "    data = data.to(device)\n",
        "\n",
        "    train_data = data[train_mask]\n",
        "    val_data = data[val_mask]\n",
        "    test_data = data[test_mask]\n",
        "\n",
        "    train_loader = TemporalDataLoader(train_data, batch_size=BATCH_SIZE)\n",
        "    val_loader = TemporalDataLoader(val_data, batch_size=BATCH_SIZE)\n",
        "    test_loader = TemporalDataLoader(test_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "    # Initialize embeddings\n",
        "    num_nodes = dataset.num_nodes\n",
        "    U, W = initialize_parameters(num_nodes, embedding_dim)\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        loss = train_with_bcd(train_loader, num_nodes, walk_length, window_size, lambda_reg, tau_reg, gamma_reg, U, W, device)\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")"
      ]
    }
  ]
}