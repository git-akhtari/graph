{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPu+IKsGcCUh8Rro20/gifc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/git-akhtari/graph/blob/main/TemporalNode2vecTgb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kp7WRs-VCqd5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "!pip install torch-geometric\n",
        "from torch_geometric.loader import TemporalDataLoader\n",
        "!pip install py-tgb\n",
        "from tgb.utils.utils import get_args\n",
        "from tgb.linkproppred.dataset_pyg import PyGLinkPropPredDataset\n",
        "from tgb.linkproppred.evaluate import Evaluator\n",
        "import random\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import os\n",
        "from google.colab import drive\n",
        "from tgb.utils.utils import set_random_seed, split_by_time, save_results\n",
        "\n",
        "import sys\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "file_path = '/content/drive/MyDrive/TGB-TEST/node2vec_walks.py'\n",
        "file_dir = os.path.dirname(file_path)\n",
        "sys.path.append(file_dir)\n",
        "import node2vec_walks\n",
        "\n",
        "\n",
        "DATA = \"tgbl-wiki\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "dataset_path = '/content/drive/MyDrive/TGB-TEST/tgbl_wiki'\n",
        "from tgb.linkproppred.dataset_pyg import PyGLinkPropPredDataset\n",
        "dataset = PyGLinkPropPredDataset(name='tgbl-wiki', root=dataset_path)\n",
        "\n",
        "\n",
        "# Generate Walks\n",
        "def generate_walks(nx_G, length, p, q):\n",
        "  G = node2vec_walks.Graph(nx_G, is_directed=False, p=p, q=q)\n",
        "##G = Graph(nx_G, is_directed=False, p=p, q=q)\n",
        "  G.preprocess_transition_probs()\n",
        "  return G.simulate_walks(num_walks=1, walk_length=length)\n",
        "\n",
        "# def generate_walks(graph, walk_length):\n",
        "#     walks = []\n",
        "#     for node in graph.nodes():\n",
        "#         walk = [node]\n",
        "#         for _ in range(walk_length - 1):\n",
        "#             neighbors = list(graph.neighbors(walk[-1]))\n",
        "#             if len(neighbors) > 0:\n",
        "#                 walk.append(np.random.choice(neighbors))\n",
        "#             else:\n",
        "#                 break\n",
        "#         walks.append(walk)\n",
        "#     return walks\n",
        "\n",
        "# def generate_walks(graph, length, p):\n",
        "# walks = []\n",
        "# for node in graph.nodes():\n",
        "#     walk = [node]\n",
        "#     while len(walk) < length:\n",
        "#         current_node = walk[-1]\n",
        "#         neighbors = list(graph.neighbors(current_node))\n",
        "#         if not neighbors:\n",
        "#             break\n",
        "#         if random.random() < p:\n",
        "#             walk.append(walk[-2] if len(walk) > 1 else random.choice(neighbors))\n",
        "#         else:\n",
        "#             next_node = random.choice(neighbors)\n",
        "#             walk.append(next_node)\n",
        "#     walks.append(walk)\n",
        "# return walks\n",
        "\n",
        "# Compute PPMI Matrix\n",
        "def compute_ppmi(walks, window_size, num_nodes):\n",
        "    cooccurrence_matrix = np.zeros((num_nodes, num_nodes))\n",
        "    for walk in walks:\n",
        "        for i, node in enumerate(walk):\n",
        "            for j in range(max(i - window_size, 0), min(i + window_size + 1, len(walk))):\n",
        "                if i != j:\n",
        "                    cooccurrence_matrix[node, walk[j]] += 1\n",
        "    row_sums = np.sum(cooccurrence_matrix, axis=1, keepdims=True)\n",
        "    ppmi_matrix = np.log((cooccurrence_matrix * np.sum(row_sums)) / (row_sums @ row_sums.T) + 1e-8)\n",
        "    ppmi_matrix[ppmi_matrix < 0] = 0\n",
        "    return torch.tensor(ppmi_matrix, dtype=torch.float32)\n",
        "\n",
        "def _bcd_step_batch(Yt, Ut, Wp, Wn, gamma, llambda, tau):\n",
        "    UtU = Ut.T @ Ut\n",
        "    r = UtU.shape[0]\n",
        "    A = UtU + (gamma + llambda + 2 * tau) * torch.eye(r, device=Ut.device)\n",
        "    B = Yt @ Ut.T + gamma * Ut.T + tau * (Wp + Wn).T\n",
        "\n",
        "    return torch.linalg.solve(A, B).T\n",
        "\n",
        "def construct_graph(batch):\n",
        "    import networkx as nx\n",
        "    G = nx.DiGraph()\n",
        "    for src, dst in zip(batch.src.cpu().numpy(), batch.dst.cpu().numpy()):\n",
        "        G.add_edge(src, dst)\n",
        "    return G\n",
        "\n",
        "# Initialize embeddings\n",
        "def initialize_parameters(num_nodes, embedding_dim):\n",
        "    U = torch.randn(num_nodes, embedding_dim, device=device, requires_grad=False)\n",
        "    W = torch.randn(num_nodes, embedding_dim, device=device, requires_grad=False)\n",
        "    return U, W\n",
        "\n",
        "def initialize_parameters_from_ppmi(PPMI, embedding_dim):\n",
        "    # Normalize PPMI rows using L1 norm\n",
        "    row_sums = PPMI.abs().sum(dim=1, keepdim=True)\n",
        "    PPMI_normalized = PPMI / row_sums\n",
        "    # Compute variance for each row\n",
        "    row_variances = PPMI_normalized.var(dim=1)\n",
        "    # Sort rows based on variance and select the top d rows\n",
        "    top_indices = torch.argsort(row_variances, descending=True)[:embedding_dim]\n",
        "    # Initialize U and W with the selected rows\n",
        "    U = PPMI_normalized[top_indices, :].clone().detach()\n",
        "    W = PPMI_normalized[top_indices, :].clone().detach()\n",
        "    return U, W\n",
        "\n",
        "def save_checkpoint(U, W, filename):\n",
        "    torch.save({\"U\": U, \"W\": W}, filename)\n",
        "    print(f\"Checkpoint saved: {filename}\")\n",
        "\n",
        "def load_checkpoint(filename, device=\"cpu\"):\n",
        "    if os.path.exists(filename):\n",
        "        checkpoint = torch.load(filename, map_location=device)\n",
        "        print(f\"Checkpoint loaded: {filename}\")\n",
        "        return checkpoint[\"U\"], checkpoint[\"W\"]\n",
        "    else:\n",
        "        print(\"No checkpoint found, starting from scratch.\")\n",
        "        return None, None\n",
        "\n",
        "# Training function with BCD\n",
        "def train_with_bcd(train_loader, PPMI_All, num_nodes, walk_length, window_size, lambda_reg, tau_reg, gamma_reg, U, W, device, save_every=5):\n",
        "    total_loss = 0\n",
        "\n",
        "    checkpoint_path = \"/content/drive/MyDrive/TGB-TEST/bcd_checkpoint.pth\"\n",
        "    U_loaded, W_loaded = load_checkpoint(checkpoint_path, device=device)\n",
        "    if U_loaded is not None and W_loaded is not None:\n",
        "        U, W = U_loaded.to(device), W_loaded.to(device)\n",
        "\n",
        "    # Initialize batch_idx before the loop\n",
        "    batch_idx = 0\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        # Build graph for current batch\n",
        "        graph = construct_graph(batch)\n",
        "\n",
        "        # Generate walks and compute PPMI\n",
        "        walks = generate_walks(graph, walk_length,0.1,0.1)\n",
        "        PPMI = compute_ppmi(walks, window_size, num_nodes)\n",
        "\n",
        "        #batch_indices_src = batch.src.cpu().numpy()\n",
        "        #batch_indices_dst = batch.dst.cpu().numpy()\n",
        "        #PPMI = PPMI_All[batch_indices_src][:, batch_indices_dst].to(device)\n",
        "\n",
        "        Wp = torch.cat([torch.zeros(1, W.shape[1], device=device), W[:-1]])\n",
        "        Wn = torch.cat([W[1:], torch.zeros(1, W.shape[1], device=device)])\n",
        "        Up = torch.cat([torch.zeros(1, U.shape[1], device=device), U[:-1]])\n",
        "        Un = torch.cat([U[1:], torch.zeros(1, U.shape[1], device=device)])\n",
        "\n",
        "        W = _bcd_step_batch(PPMI, U, Wp, Wn, gamma_reg, lambda_reg, tau_reg)\n",
        "        U = _bcd_step_batch(PPMI, W, Up, Un, gamma_reg, lambda_reg, tau_reg)\n",
        "\n",
        "        loss = torch.norm(PPMI - U @ W.T, p='fro')  # Frobenius norm\n",
        "        total_loss += loss.item()\n",
        "        print('---------------bcd-------------------')\n",
        "        print(f\"Batch {batch_idx + 1}/{len(train_loader)} - Loss: {loss.item()}\")\n",
        "        print(\"U @ W.T:\", U @ W.T)\n",
        "        if (batch_idx + 1) % save_every == 0:\n",
        "            save_checkpoint(U, W, checkpoint_path)\n",
        "\n",
        "        # Increment batch_idx after each iteration\n",
        "        batch_idx += 1\n",
        "\n",
        "    save_checkpoint(U, W, checkpoint_path)\n",
        "    return (total_loss / len(train_loader)), U\n",
        "\n",
        "def train(embeddings, train_loader, device):\n",
        "    X, y = [], []\n",
        "\n",
        "    for batch in train_loader:\n",
        "        batch = batch.to(device)\n",
        "\n",
        "        src, pos_dst, t, msg = batch.src, batch.dst, batch.t, batch.msg\n",
        "\n",
        "        neg_dst = torch.randint(\n",
        "            min_dst_idx,\n",
        "            max_dst_idx + 1,\n",
        "            (src.size(0),),\n",
        "            dtype=torch.long,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        for i in range(src.size(0)):\n",
        "            pos_feature_vector = np.concatenate([embeddings[src[i].item()], embeddings[pos_dst[i].item()]])\n",
        "            X.append(pos_feature_vector)\n",
        "            y.append(1)\n",
        "\n",
        "        for i in range(src.size(0)):\n",
        "            neg_feature_vector = np.concatenate([embeddings[src[i].item()], embeddings[neg_dst[i].item()]])\n",
        "            X.append(neg_feature_vector)\n",
        "            y.append(0)\n",
        "\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X, y)\n",
        "\n",
        "    print('---------------- Logistic Regression Trained ------------------')\n",
        "    return model\n",
        "\n",
        "def val_test(model, embeddings, loader, neg_sampler, evaluator, split_mode):\n",
        "    perf_list = []\n",
        "\n",
        "    for pos_batch in loader:\n",
        "        pos_src, pos_dst, pos_t, pos_msg = (\n",
        "            pos_batch.src,\n",
        "            pos_batch.dst,\n",
        "            pos_batch.t,\n",
        "            pos_batch.msg,\n",
        "        )\n",
        "\n",
        "        neg_batch_list = neg_sampler.query_batch(pos_src, pos_dst, pos_t, split_mode=split_mode)\n",
        "\n",
        "        for idx, neg_batch in enumerate(neg_batch_list):\n",
        "            src = pos_src[idx].item()\n",
        "            pos_dst_node = pos_dst[idx].item()\n",
        "            neg_dst_nodes = neg_batch\n",
        "\n",
        "            pos_feature_vector = np.concatenate([embeddings[src], embeddings[pos_dst_node]]).reshape(1, -1)\n",
        "            #pos_feature_vector = torch.cat([embeddings[src], embeddings[pos_dst_node]]).cpu().numpy().reshape(1, -1)\n",
        "            y_pred_pos = model.predict_proba(pos_feature_vector)[:, 1]\n",
        "\n",
        "            neg_feature_vectors = np.array([np.concatenate([embeddings[src], embeddings[neg]]) for neg in neg_dst_nodes])\n",
        "            #neg_feature_vectors = np.array([torch.cat([embeddings[src], embeddings[neg]]).cpu().numpy() for neg in neg_dst_nodes])\n",
        "            y_pred_neg = model.predict_proba(neg_feature_vectors)[:, 1]\n",
        "\n",
        "            input_dict = {\n",
        "                \"y_pred_pos\": np.array([y_pred_pos.squeeze()]),\n",
        "                \"y_pred_neg\": np.array(y_pred_neg),\n",
        "                \"eval_metric\": [metric],\n",
        "            }\n",
        "            perf_list.append(evaluator.eval(input_dict)[metric])\n",
        "            print('-------------val/test---------------------')\n",
        "\n",
        "    perf_metrics = float(torch.tensor(perf_list).mean())\n",
        "\n",
        "    return perf_metrics\n",
        "\n",
        "# Main script\n",
        "if __name__ == \"__main__\":\n",
        "    # Parameters\n",
        "    DATA = \"tgbl-wiki\"\n",
        "    BATCH_SIZE = 200\n",
        "    embedding_dim = 20\n",
        "    teta = 1\n",
        "    window_size = 5\n",
        "    walk_length = 3\n",
        "    lambda_reg = 0.1\n",
        "    tau_reg = 0.1\n",
        "    gamma_reg = 0.1\n",
        "    num_epochs = 1\n",
        "    p = 0.1\n",
        "    q = 0.1\n",
        "\n",
        "    # Device setup\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Load dataset\n",
        "    #dataset = PyGLinkPropPredDataset(name=DATA, root=\"datasets\")\n",
        "    train_mask = dataset.train_mask\n",
        "    val_mask = dataset.val_mask\n",
        "    test_mask = dataset.test_mask\n",
        "    data = dataset.get_TemporalData()\n",
        "    data = data.to(device)\n",
        "    num_nodes = dataset.num_nodes\n",
        "\n",
        "    # Ensure to only sample actual destination nodes as negatives.\n",
        "    min_dst_idx, max_dst_idx = int(data.dst.min()), int(data.dst.max())\n",
        "\n",
        "    train_data = data[train_mask]\n",
        "    val_data = data[val_mask]\n",
        "    test_data = data[test_mask]\n",
        "\n",
        "    train_loader = TemporalDataLoader(train_data, batch_size=BATCH_SIZE)\n",
        "    val_loader = TemporalDataLoader(val_data, batch_size=BATCH_SIZE)\n",
        "    test_loader = TemporalDataLoader(test_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "    metric = dataset.eval_metric\n",
        "    evaluator = Evaluator(name=DATA)\n",
        "    neg_sampler = dataset.negative_sampler\n",
        "\n",
        "    train_list = split_by_time(train_data)\n",
        "    # Training loop over T snapshots\n",
        "    num_snapshots = len(train_list)\n",
        "    Embeddings_list = []\n",
        "\n",
        "    for t in range(num_snapshots):  # For each time snapshot\n",
        "        print(f\"Processing snapshot {t + 1}/{num_snapshots}...\")\n",
        "        train_data_t = train_list[t]\n",
        "        graph = construct_graph(train_data_t)\n",
        "        walks = generate_walks(graph, walk_length, p, q)\n",
        "        PPMI = compute_ppmi(walks, window_size, num_nodes).to(device)\n",
        "\n",
        "        # Initialize embeddings using the ppmi\n",
        "        U, W = initialize_parameters_from_ppmi(PPMI, embedding_dim)\n",
        "\n",
        "        # Training loop for embedding\n",
        "        for epoch in range(num_epochs):\n",
        "            loss, U = train_with_bcd(train_data_t, PPMI, num_nodes, walk_length, window_size, lambda_reg, tau_reg, gamma_reg, U, W, device)\n",
        "            print(f\"Snapshot {t+1}, Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")\n",
        "            Embeddings_list.append(U)\n",
        "\n",
        "            # KL Divergence\n",
        "            if t > 0:\n",
        "                U_t_prob = torch.nn.functional.softmax(U, dim=1)  # U_t normalization\n",
        "                U_t1_prob = torch.nn.functional.softmax(Embeddings_list[-1], dim=1)  # U_{t-1} normalization\n",
        "                kl_loss = torch.nn.functional.kl_div(U_t1_prob.log(), U_t_prob, reduction='batchmean')\n",
        "                loss += beta * kl_loss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        print(f\"Snapshot {t+1}, Epoch {epoch + 1}/{num_epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    Embeddings_list.append(U.clone().detach())\n",
        "\n",
        "    model = train(Embeddings_list, train_loader, device)\n",
        "\n",
        "    # validation\n",
        "    # loading the validation negative samples\n",
        "    dataset.load_val_ns()\n",
        "    perf_metric_val = val_test(model, Embeddings_list, val_loader, neg_sampler, evaluator, split_mode=\"val\")\n",
        "    print(f\"\\tValidation {metric}: {perf_metric_val: .4f}\")\n",
        "\n",
        "    # test\n",
        "    # loading the test negative samples\n",
        "    dataset.load_test_ns()\n",
        "    perf_metric_test = val_test(model, Embeddings_list, test_loader, neg_sampler, evaluator, split_mode=\"test\")\n",
        "    print(f\"INFO: Test: Evaluation Setting: >>> ONE-VS-MANY <<< \")\n",
        "    print(f\"\\tTest: {metric}: {perf_metric_test: .4f}\")\n",
        "\n"
      ]
    }
  ]
}